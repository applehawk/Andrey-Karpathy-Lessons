{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "dual_sq1",
            "metadata": {},
            "source": [
                "# Этап 13: SmoothQuant (Dual-Track) — Укрощение выбросов\n",
                "\n",
                "**SmoothQuant** (arXiv:2211.10438) переносит сложность квантования с активаций на веса. \n",
                "Это критически важно для Llama, где выбросы (outliers) могут быть в 100 раз больше средних значений.\n",
                "\n",
                "### Идея:\n",
    "$Y = (X diag(s)^{-1}) \cdot (diag(s) W)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual_sq2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from src.model import GPTLanguageModel, device, get_batch\n",
                "\n",
                "# 1. Загружаем модели\n",
                "nanogpt = GPTLanguageModel().to(device)\n",
                "nanogpt.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "\n",
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "llama = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "def smooth_quant_layer(layer, act_max, alpha=0.5):\n",
                "    with torch.no_grad():\n",
                "        # act_max: максимум активаций по каждому каналу\n",
                "        weight_max = layer.weight.abs().max(dim=0)[0].float()\n",
                "        s = act_max.pow(alpha) / (weight_max.pow(1-alpha) + 1e-6)\n",
                "        \n",
                "        # Масштабируем веса (умножаем столбцы)\n",
                "        layer.weight.data *= s.view(1, -1).to(layer.weight.dtype)\n",
                "        return s\n",
                "\n",
                "# 2. nanoGPT: Сглаживаем первый слой MLP\n",
                "xb, _ = get_batch('val') # Проп: (B, T, C)\n",
                "nano_acts_max = torch.randn(384).to(device).abs() * 10.0 # Симуляция активаций\n",
                "nano_layer = nanogpt.blocks[0].ffwd.net[0]\n",
                "s_nano = smooth_quant_layer(nano_layer, nano_acts_max)\n",
                "print(f\"nanoGPT: SmoothQuant applied. Max scale: {s_nano.max().item():.2f}\")\n",
                "\n",
                "# 3. Llama: Сглаживаем gate_proj\n",
                "# В Llama выбросы в активациях достигают огромных масштабов\n",
                "llama_acts_max = torch.randn(2048).to(device).abs().half() * 50.0 # Симуляция выбросов Llama\n",
                "llama_layer = llama.model.layers[0].mlp.gate_proj\n",
                "s_llama = smooth_quant_layer(llama_layer, llama_acts_max)\n",
                "print(f\"Llama: SmoothQuant applied. Max scale: {s_llama.max().item():.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}