{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "kv1",
            "metadata": {},
            "source": [
                "# Этап 16: Квантование KV-кеша (Dual-Track)\n",
                "\n",
                "При генерации текста мы используем **KV-кеш**, чтобы не пересчитывать ключи и значения для предыдущих токенов. В больших моделях этот кеш занимает огромный объем памяти. Согласно разделу 3.2.3 обзора, квантование кеша — ключ к поддержке длинных контекстов.\n",
                "\n",
                "### Проблема:\n",
                "Если модель 7B занимает 14 ГБ в FP16, то при контексте 32k токенов её KV-кеш может занять еще столько же. Это удваивает требования к памяти."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kv2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn.functional as F\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from src.model import GPTLanguageModel, device\n",
                "\n",
                "# 1. Загружаем модели\n",
                "nanogpt = GPTLanguageModel().to(device)\n",
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "llama = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
                "\n",
                "def pseudo_quantize_cache(cache_tensor, bits=8):\n",
                "    # Квантование кеша (обычно по каждому токену)\n",
                "    # tensor shape: (B, num_heads, seq_len, head_dim)\n",
                "    q_min, q_max = - (2**(bits-1)), 2**(bits-1) - 1\n",
                "    \n",
                "    # Считаем размах для каждого вектора в кеше\n",
                "    scale = (cache_tensor.abs().max(dim=-1, keepdim=True)[0]) / q_max\n",
                "    q_x = torch.round(cache_tensor / (scale + 1e-6)).clamp(q_min, q_max)\n",
                "    \n",
                "    # Возвращаем «деквантованный» тензор для симуляции потерь\n",
                "    return q_x * scale\n",
                "\n",
                "print(\"Функция квантования кеша готова.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kv3",
            "metadata": {},
            "source": [
                "### nanoGPT: Как бы это выглядело\n",
                "\n",
                "В nanoGPT мы вручную создаем «кеш» для векторов K и V из одной головы внимания и квантуем его."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kv4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Симулируем векторы K, созданные в Head.forward (src/model.py:103)\n",
                "B, T, hs = 1, 256, 64\n",
                "k_vec = torch.randn(B, T, hs).to(device)\n",
                "\n",
                "print(f\"Размер оригинального K-кеша: {k_vec.nelement() * k_vec.element_size()} байт\")\n",
                "\n",
                "k_quant = pseudo_quantize_cache(k_vec, bits=4)\n",
                "error = torch.mean((k_vec - k_quant)**2)\n",
                "\n",
                "print(f\"Ошибка после 4-битного квантования кеша: {error.item():.6f}\")\n",
                "print(f\"Экономия памяти: 8-кратная (с FP32 до 4-bit)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kv5",
            "metadata": {},
            "source": [
                "### Llama 3: Реальный KV-кеш\n",
                "\n",
                "В Llama 3 структура кеша сложнее. Мы можем перехватить кеш прямо во время генерации.\n",
                "Библиотека `transformers` позволяет возвращать `past_key_values`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kv6",
            "metadata": {},
            "outputs": [],
            "source": [
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "inputs = tokenizer(\"The quick brown fox\", return_tensors=\"pt\").to(device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = llama(**inputs, use_cache=True)\n",
                "    # past_key_values: кортеж (layer_count) x 2 (K, V) x (B, H, T, hs)\n",
                "    pkv = outputs.past_key_values\n",
                "    \n",
                "    # Возьмем Keys первого слоя\n",
                "    llama_kv_example = pkv[0][0]\n",
                "    print(f\"Форма KV-кеша Llama (Layers[0], Keys): {llama_kv_example.shape}\")\n",
                "    \n",
                "    # Симулируем 8-битное квантование всего кеша модели\n",
                "    llama_kv_quant = pseudo_quantize_cache(llama_kv_example, bits=8)\n",
                "    \n",
                "    original_mem = llama_kv_example.nelement() * llama_kv_example.element_size()\n",
                "    print(f\"Память кеша одного слоя (FP16): {original_mem / 1024:.1f} KB\")\n",
                "    print(f\"С 8-битным квантованием (INT8): {original_mem / 2048:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kv7",
            "metadata": {},
            "source": [
                "### Итог:\n",
                "Квантование кеша (особенно до 4 бит) позволяет запускать Llama на устройствах с малым объемом памяти при очень длинных диалогах."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}