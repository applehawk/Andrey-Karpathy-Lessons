{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "kd_wb_header",
            "metadata": {},
            "source": [
                "# Урок 9: White-box Knowledge Distillation\n",
                "\n",
                "Самый плотный перенос знаний через доступ к логитам (вероятностям) и скрытым состояниям Учителя.\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. Logit Distillation\n",
                "Мы учим Студента предсказывать не только правильное слово, но и «почти правильные» слова (распределение вероятностей).\n",
                "$$\\mathcal{L}_{KD} = \\text{KL}(P_{Teacher}^\\tau || Q_{Student}^\\tau)$$\n",
                "Где $\\tau$ - температура, увеличивающая «мягкость» распределения.\n",
                "\n",
                "### 1.2. Методы из обзора:\n",
                "*   **MiniLLM (Gu et al., 2024):** Предлагает использовать **Reverse KL**. Это заставляет маленького Студента фокусироваться на самых вероятных модах Учителя, предотвращая «размазывание» знаний, которое ведет к галлюцинациям.\n",
                "*   **GKD (Agarwal et al., 2024):** Использует дистилляцию на «собственных» ошибках студента (On-policy), что резко повышает стабильность генерации длинных текстов.\n",
                "*   **TED (Liang et al., 2023):** Метод «Явной дистилляции», при котором Студент обучается точно копировать активации конкретных слоев Учителя, выбранных на основе их важности для задачи.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kd_wb_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.nn.functional as F\n",
                "\n",
                "def calculate_kl_raw(s_logits, t_logits, tau=2.0):\n",
                "    p = F.softmax(t_logits / tau, dim=-1)\n",
                "    log_q = F.log_softmax(s_logits / tau, dim=-1)\n",
                "    return F.kl_div(log_q, p, reduction='batchmean') * (tau**2)\n",
                "\n",
                "print(\"nanoGPT Track: Реализована математика KL-дивергенции с температурой.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kd_wb_industrial_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: Custom KDTrainer\n",
                "В индустрии KD интегрируется в стандартные Trainer'ы через переопределение метода `compute_loss`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kd_wb_industrial_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "from transformers import Trainer\n",
                "class KDTrainer(Trainer):\n",
                "    def compute_loss(self, model, inputs, return_outputs=False):\n",
                "        s_out = model(**inputs)\n",
                "        with torch.no_grad(): t_out = self.teacher(**inputs)\n",
                "        loss = s_out.loss + calculate_kl_raw(s_out.logits, t_out.logits)\n",
                "        return (loss, s_out) if return_outputs else loss\n",
                "\"\"\"\n",
                "print(\"Llama Track: White-box дистилляция через кастомные функции потерь.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}