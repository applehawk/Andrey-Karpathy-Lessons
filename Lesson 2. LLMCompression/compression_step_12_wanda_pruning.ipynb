{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "dual_wanda1",
            "metadata": {},
            "source": [
                "# Этап 12: Wanda (Dual-Track) — nanoGPT vs Llama\n",
                "\n",
                "Метод **Wanda** (arXiv:2401.18079) использует и веса, и активации для оценки важности. \n",
                "Здесь мы применим его параллельно к обеим моделям.\n",
                "\n",
                "### Формула важности:\n",
                "$$Score_{i,j} = |W_{i,j}| \\cdot \\|X_j\\|_2$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual_wanda2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from src.model import GPTLanguageModel, device, get_batch\n",
                "\n",
                "# 1. Загружаем модели\n",
                "nanogpt = GPTLanguageModel().to(device)\n",
                "nanogpt.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "\n",
                "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "llama = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "\n",
                "def get_activation_norms(model, layer, input_data, is_llama=False):\n",
                "    norms = []\n",
                "    def hook(module, input, output):\n",
                "        x = input[0].detach().float()\n",
                "        # Для Llama input может иметь другую форму или тип\n",
                "        norm = torch.norm(x, p=2, dim=(0, 1))\n",
                "        norms.append(norm)\n",
                "    \n",
                "    handle = layer.register_forward_hook(hook)\n",
                "    with torch.no_grad():\n",
                "        if is_llama:\n",
                "            model(input_data)\n",
                "        else:\n",
                "            model(input_data)\n",
                "    handle.remove()\n",
                "    return norms[0]\n",
                "\n",
                "def apply_wanda(layer, norms, sparsity=0.5):\n",
                "    with torch.no_grad():\n",
                "        W = layer.weight.data.float()\n",
                "        score = W.abs() * norms.view(1, -1)\n",
                "        k = int(W.size(1) * sparsity)\n",
                "        thresholds, _ = torch.kthvalue(score, k, dim=1)\n",
                "        mask = score > thresholds.view(-1, 1)\n",
                "        layer.weight.data *= mask.to(layer.weight.dtype)\n",
                "        return mask\n",
                "\n",
                "# 2. Применяем Wanda к nanoGPT\n",
                "xb, _ = get_batch('val')\n",
                "nano_layer = nanogpt.blocks[0].ffwd.net[0]\n",
                "nano_norms = get_activation_norms(nanogpt, nano_layer, xb)\n",
                "apply_wanda(nano_layer, nano_norms)\n",
                "print(\"nanoGPT: Wanda applied to blocks[0].ffwd\")\n",
                "\n",
                "# 3. Применяем Wanda к Llama\n",
                "prompt = \"The concept of model compression is\"\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n",
                "llama_layer = llama.model.layers[0].mlp.gate_proj\n",
                "llama_norms = get_activation_norms(llama, llama_layer, inputs, is_llama=True)\n",
                "apply_wanda(llama_layer, llama_norms)\n",
                "print(\"Llama: Wanda applied to layers[0].mlp.gate_proj\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}