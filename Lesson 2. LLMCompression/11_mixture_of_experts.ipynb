{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "moe_header",
            "metadata": {},
            "source": [
                "# Урок 11: Mixture-of-Experts (MoE) Compression\n",
                "\n",
                "MoE — это не только способ масштабирования моделей до триллионов параметров, но и мощный инструмент сжатия (через динамическую разреженность).\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. Conditional Computation (Условные вычисления)\n",
                "В MoE слое мы заменяем одну большую полносвязную сеть (MLP) на набор из $N$ маленьких «экспертов». Для каждого входного токена специальный слой-маршрутизатор (Router/Gate) выбирает только $k$ лучших экспертов:\n",
                "$$y = \\sum_{i=1}^k G(x)_i E_i(x)$$\n",
                "Это позволяет модели иметь 100 млрд параметров (для хранения знаний), но использовать только 10 млрд для обработки каждого токена, что в 10 раз снижает FLOPs.\n",
                "\n",
                "### 1.2. Методы сжатия из обзора:\n",
                "*   **MoE-ification (Zhang et al., 2022):** Процесс превращения уже обученной плотной (dense) модели в MoE. Мы берем MLP слой и делим его нейроны на кластеры («экспертов»). \n",
                "*   **Sparse Upcycling (Komatsuzaki et al., 2023):** Метод инициализации MoE модели весами плотной модели с последующим дообучением. Это позволяет Hugging Face и NVIDIA выпускать MoE-версии своих моделей гораздо дешевле.\n",
                "*   **Expert Pruning:** Удаление избыточных экспертов, которые редко выбираются маршрутизатором.\n",
                "*   **QMoE (Frantar et al., 2024):** Квантование MoE моделей. Из-за огромного количества параметров MoE модели (например, Mixtral 8x7B) требуют экстремального сжатия до 2-3 бит для запуска на потребительском железе.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "moe_impl_raw",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class Expert(nn.Module):\n",
                "    def __init__(self, n_embd):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(n_embd, 4 * n_embd),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(4 * n_embd, n_embd),\n",
                "        )\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class MoELayer(nn.Module):\n",
                "    def __init__(self, n_embd, num_experts=8, top_k=2):\n",
                "        super().__init__()\n",
                "        self.experts = nn.ModuleList([Expert(n_embd) for _ in range(num_experts)])\n",
                "        self.gate = nn.Linear(n_embd, num_experts)\n",
                "        self.top_k = top_k\n",
                "\n",
                "    def forward(self, x):\n",
                "        # x: (B, T, C)\n",
                "        logits = self.gate(x) # (B, T, num_experts)\n",
                "        weights, indices = torch.topk(logits, self.top_k, dim=-1)\n",
                "        weights = F.softmax(weights, dim=-1)\n",
                "        \n",
                "        out = torch.zeros_like(x)\n",
                "        for i, expert in enumerate(self.experts):\n",
                "            # Маска: где этот эксперт был выбран\n",
                "            mask = (indices == i).any(dim=-1)\n",
                "            if mask.any():\n",
                "                # Суммируем вклад эксперта с учетом веса из гейта\n",
                "                batch_idx, token_idx = torch.where(mask)\n",
                "                # Для простоты реализации представим логику взвешивания:\n",
                "                # На практике используются более эффективные scatter/gather операции\n",
                "                out[mask] += expert(x[mask]) # Здесь пропущено умножение на weight для краткости\n",
                "        \n",
                "        return out\n",
                "\n",
                "print(\"nanoGPT Track: Реализован MoE слой с маршрутизацией Top-k.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "moe_industrial_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: Mixtral & Megablocks\n",
                "В индустрии MoE используется для создания моделей с «дешевым инференсом». Например, **Mixtral 8x7B** имеет 47B параметров, но использует только 13B для каждого токена. Для эффективного обучения MoE используют библиотеку **Megablocks**, которая реализует «блочную разреженность» (Block-Sparse) для обхода проблем с производительностью стандартных слоев."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "moe_industrial_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM\n",
                "try:\n",
                "    # Пример загрузки Mixtral — самой известной MoE модели\n",
                "    model_id = \"mistralai/Mixtral-8x7B-v0.1\"\n",
                "    # В индустрии MoE + Quantization — это стандарт доставки\n",
                "    # model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n",
                "    print(\"Llama/Mistral Track: MoE позволяет запускать гигантские модели с ценой маленьких.\")\n",
                "except Exception as e:\n",
                "    print(f\"Ошибка: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}