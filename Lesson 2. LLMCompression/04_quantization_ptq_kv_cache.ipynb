{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "kv_header",
            "metadata": {},
            "source": [
                "# Урок 4: KV-Cache Quantization\n",
                "\n",
                "При обработке длинных контекстов память GPU забивается не весами модели, а кэшем ключей (Keys) и значений (Values). Сжатие кэша позволяет увеличить контекст с 8k до 128k без покупки новых GPU.\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. Расчет объема\n",
                "$$Size_{KV} = 2 \\cdot L \\cdot H \\cdot d \\cdot S \\cdot P$$\n",
                "Где: $L$ - слои, $H$ - головы, $d$ - размер головы, $S$ - длина контекста, $P$ - точность (байт). \n",
                "Для Llama-8B это ~2 ГБ на каждые 8к токенов.\n",
                "\n",
                "### 1.2. Методы из обзора:\n",
                "*   **KVQuant (Hooper et al., 2024):** Применяет «неравномерное» квантование. Метод использует калибровку, чтобы найти оптимальные уровни квантования для каждого канала Keys, учитывая их специфическое распределение.\n",
                "*   **KIVI (Liu et al., 2024):** Асимметричное решение. Метод квантует Keys по каналам (per-channel), а Values — по токенам (per-token). Это позволяет сжать кэш до 2 бит при сохранении высокого качества генерации.\n",
                "*   **WKVQuant (Yue et al., 2024):** Использует кросс-блочную регуляризацию, учитывая, как ошибка квантования кэша в одном слое влияет на последующие слои внимания.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kv_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from src.model import GPTLanguageModel\n",
                "\n",
                "def estimate_kv_cache_size_bytes(seq_len, n_layer=6, n_head=6, n_embd=384, precision=4):\n",
                "    # 2 (K и V) * layers * head_dim * seq_len * precision_bytes\n",
                "    head_dim = n_embd // n_head\n",
                "    return 2 * n_layer * n_head * head_dim * seq_len * (precision / 8)\n",
                "\n",
                "print(f\"nanoGPT KV-Cache (1024 tokens, 4-bit): {estimate_kv_cache_size_bytes(1024)/1024:.1f} KB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kv_industrial_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: Transformers Quantized Cache\n",
                "В новых версиях `transformers` появилась поддержка 4-битного квантования кэша «из коробки»."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kv_industrial_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import AutoModelForCausalLM, QuantizedCacheConfig\n",
                "try:\n",
                "    cache_config = QuantizedCacheConfig(nbits=4, axis=0)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\"tiny-llama\", torch_dtype=torch.float16)\n",
                "    # Во время генерации просто указываем cache_implementation\n",
                "    # model.generate(..., cache_implementation=\"quantized\", cache_config=cache_config)\n",
                "    print(\"Llama Track: Поддержка 4-битного кэша (QuantizedCache) сконфигурирована.\")\n",
                "except Exception as e:\n",
                "    print(f\"Ошибка: {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}