{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "qat_header",
            "metadata": {},
            "source": [
                "# Урок 1: Quantization-Aware Training (QAT)\n",
                "\n",
                "В этом уроке мы разберем «золотой стандарт» квантования — обучение с учетом ограничений точности. В отличие от Post-Training Quantization (PTQ), где мы сжимаем уже готовую модель, QAT позволяет нейросети «адаптироваться» к шуму квантования в процессе обучения.\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. Проблема дифференцируемости\n",
                "Операция квантования (округления) является ступенчатой функцией:\n",
                "$$q(x) = \\text{round}(x / s) \\cdot s$$\n",
                "Производная такой функции везде равна нулю (или не определена в точках скачка). Это делает стандартный метод обратного распространения ошибки (Backpropagation) невозможным.\n",
                "\n",
                "### 1.2. Straight-Through Estimator (STE)\n",
                "Для решения проблемы используется «трюк» STE: во время прямого прохода (forward) мы используем квантованные веса, а во время обратного прохода (backward) притворяемся, что квантования не было, и пропускаем градиент без изменений:\n",
                "$$\\frac{\\partial \\mathcal{L}}{\\partial x} \\approx \\frac{\\partial \\mathcal{L}}{\\partial q(x)}$$\n",
                "\n",
                "### 1.3. Методы из обзора:\n",
                "*   **LLM-QAT (Liu et al., 2023b):** Предлагает использовать дистилляцию данных (Data-free) — модель-учитель генерирует тексты, на которых обучается квантованная модель-ученик. Это решает проблему отсутствия чистых данных для дообучения.\n",
                "*   **BitDistiller (Du et al., 2024):** Фокусируется на стабильности при экстремально низких битах (ниже 4-х). Использует асимметричное квантование и доверительную дистилляцию (Confidence-Aware KL-Divergence).\n",
                "*   **OneBit (Xu et al., 2024):** Подход для 1-битного сжатия. Веса представляются как $W \\approx S \\cdot B$, где $B \\in \\{-1, 1\\}^{M \\times N}$ — бинарная матрица, а $S$ — обучаемый вектор масштаба высокого разрешения.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qat_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from src.model import GPTLanguageModel, device\n",
                "\n",
                "class STEQuantize(torch.autograd.Function):\n",
                "    @staticmethod\n",
                "    def forward(ctx, input, scale):\n",
                "        q_x = torch.round(input / scale).clamp(-128, 127)\n",
                "        return q_x * scale\n",
                "    @staticmethod\n",
                "    def backward(ctx, grad_output):\n",
                "        return grad_output, None\n",
                "\n",
                "class QATLinear(nn.Linear):\n",
                "    def __init__(self, *args, **kwargs):\n",
                "        super().__init__(*args, **kwargs)\n",
                "        self.scale = nn.Parameter(torch.tensor(0.01))\n",
                "    def forward(self, x):\n",
                "        q_weight = STEQuantize.apply(self.weight, self.scale.abs() + 1e-6)\n",
                "        return F.linear(x, q_weight, self.bias)\n",
                "\n",
                "print(\"nanoGPT Track: Реализована 'сырая' логика QAT через Linear Algebra & STE.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "qat_peft_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: QLoRA\n",
                "В современной индустрии полноценный QAT для моделей 7B+ слишком дорог. Вместо него используют **QLoRA** — комбинацию 4-битного квантования (NF4) и низкоранговых адаптеров (LoRA). Адаптеры обучаются в FP16, фактически выполняя роль QAT-коррекции."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "qat_peft_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import BitsAndBytesConfig, AutoModelForCausalLM\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "try:\n",
                "    model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    \n",
                "    config = LoraConfig(\n",
                "        r=8, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"],\n",
                "        lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
                "    )\n",
                "    model = get_peft_model(model, config)\n",
                "    model.print_trainable_parameters()\n",
                "    print(\"Llama Track: Промышленный QLoRA пайплайн (PEFT) готов.\")\n",
                "except Exception as e:\n",
                "    print(f\"Ошибка (нужен GPU): {e}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}