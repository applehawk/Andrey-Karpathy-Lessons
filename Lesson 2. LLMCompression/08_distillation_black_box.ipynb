{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "kd_bb_header",
            "metadata": {},
            "source": [
                "# Урок 8: Black-box Knowledge Distillation\n",
                "\n",
                "Перенос знаний от большой модели (Teacher) к маленькой (Student), используя только её текстовые ответы. Это ключ к созданию мобильных версий Llama и Mistral.\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. Имитация поведения\n",
                "Студент минимизирует ошибку предсказания текста, сгенерированного Учителем:\n",
                "$$\\mathcal{L} = -\\sum y_{teacher} \\log(y_{student})$$\n",
                "\n",
                "### 1.2. Методы из обзора:\n",
                "*   **Chain-of-Thought (CoT) Distillation:**\n",
                "    *   **Distilling Step-by-Step (Hsieh et al., 2023):** Учитель выдает не только ответ, но и «рассуждение» (Rationale). Студент учится предсказывать обе части, что дает маленькой модели 770M качество, сравнимое с 540B на сложных задачах.\n",
                "    *   **SCOTT (Wang et al., 2023a):** Исследует состязательную дистилляцию рассуждений.\n",
                "*   **Instruction Following:**\n",
                "    *   **SELF-INSTRUCT (Wang et al., 2023d):** Процесс, когда LLM сама генерирует для себя задачи и ответы, превращая «сырой» текст в обучающий датасет для инструкций.\n",
                "    *   **Lion (Jiang et al., 2023):** Использует обратную связь. Учитель критикует ответы студента, исправляет их и отдает обратно для обучения.\n",
                "*   **In-Context Learning (ICL) Distillation:** Переносит способность модели учиться «на лету» из промптов учителя в саму архитектуру студента.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kd_bb_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "def cross_entropy_raw(student_logits, teacher_target_ids):\n",
                "    return torch.nn.functional.cross_entropy(student_logits.view(-1, 65), teacher_target_ids.view(-1))\n",
                "\n",
                "print(\"nanoGPT Track: Реализована базовая функция потерь для дистилляции ответов.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "kd_bb_industrial_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: Distilabel & Alignment Handbook\n",
                "В индустрии для этого используют фреймворки `distilabel` (от Argilla) для генерации данных и `trl` (SFTTrainer) для самого обучения."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kd_bb_industrial_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "from trl import SFTTrainer\n",
                "# dataset содержит синтетические ответы GPT-4 (Teacher)\n",
                "trainer = SFTTrainer(model=\"llama-1b\", train_dataset=distilled_dataset)\n",
                "\"\"\"\n",
                "print(\"Llama Track: Интеграция дистилляции в SFT пайплайн — стандарт де-факто.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}