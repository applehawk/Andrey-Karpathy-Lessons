{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "kd1",
            "metadata": {},
            "source": [
                "# Этап 10: Knowledge Distillation (Дистилляция знаний)\n",
                "\n",
                "Дистилляция — это способ передать знания от большой и точной модели (Teacher) к маленькой и быстрой (Student). Студент учится не только на правильных ответах (hard targets), но и на «уверенности» учителя (soft targets).\n",
                "\n",
                "### Идея метода:\n",
                "Учитель говорит: «Это с вероятностью 90% символ 'A' и 9% символ 'B'». Студент пытается повторить именно это распределение вероятностей, что дает ему гораздо больше информации, чем просто буква 'A'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "kd2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from src.model import GPTLanguageModel, device, get_batch\n",
                "\n",
                "# 1. Загружаем Учителя (уже обученная модель)\n",
                "teacher = GPTLanguageModel().to(device)\n",
                "teacher.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "teacher.eval() # Учитель всегда в режиме eval\n",
                "\n",
                "# 2. Создаем Студента (модель поменьше)\n",
                "# В реальной жизни мы бы уменьшили n_layer или n_embd в src.model.py\n",
                "# Для примера просто создадим новую копию и будем ее учить\n",
                "student = GPTLanguageModel().to(device)\n",
                "student.train()\n",
                "\n",
                "def distillation_loss(student_logits, teacher_logits, labels, T=2.0, alpha=0.5):\n",
                "    # Soft loss (KL Divergence)\n",
                "    soft_targets = F.softmax(teacher_logits / T, dim=-1)\n",
                "    soft_prob = F.log_softmax(student_logits / T, dim=-1)\n",
                "    distillation_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean') * (T**2)\n",
                "    \n",
                "    # Hard loss (обычный Cross Entropy)\n",
                "    student_loss = F.cross_entropy(student_logits.view(-1, student_logits.size(-1)), labels.view(-1))\n",
                "    \n",
                "    return alpha * distillation_loss + (1 - alpha) * student_loss\n",
                "\n",
                "optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4)\n",
                "\n",
                "# Цикл дистилляции (одна итерация)\n",
                "xb, yb = get_batch('train')\n",
                "with torch.no_grad():\n",
                "    teacher_logits, _ = teacher(xb)\n",
                "\n",
                "student_logits, _ = student(xb)\n",
                "loss = distillation_loss(student_logits, teacher_logits, yb)\n",
                "\n",
                "optimizer.zero_grad()\n",
                "loss.backward()\n",
                "optimizer.step()\n",
                "\n",
                "print(f\"Loss дистилляции: {loss.item():.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}