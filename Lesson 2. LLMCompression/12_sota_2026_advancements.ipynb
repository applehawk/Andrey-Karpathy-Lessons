{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "sota_2026_header",
            "metadata": {},
            "source": [
                "# Урок 12: 2025-2026 SOTA Advancements\n",
                "\n",
                "В этом уроке мы разберем «передний край» технологий сжатия, которые стали стандартом в 2025-2026 годах. Мы переходим от простого квантования готовых моделей к обучению изначально сжатых архитектур и динамическому управлению контекстом.\n",
                "\n",
                "## 1. Математическая теория\n",
                "\n",
                "### 1.1. BitNet b1.58 (Ternary Quantization)\n",
                "Главный прорыв 2025 года — отказ от классических битов. Вместо того чтобы квантовать FP16 в INT4, **BitNet** предлагает использовать веса $\\in \\{-1, 0, 1\\}$. \n",
                "Математически это $\\log_2(3) \\approx 1.58$ бит. \n",
                "Преимущество: Умножение матрицы на вектор заменяется простым сложением и вычитанием, что дает колоссальную экономию энергии на кристалле.\n",
                "\n",
                "### 1.2. FOEM (First-Order Error Matters) [2025]\n",
                "Классический GPTQ минимизирует ошибку весов. FOEM идет дальше: он использует градиент первого порядка (якобиан) функции потерь, чтобы понять, какие изменения веса сильнее всего бьют по качеству ответа. Это позволяет достичь качества FP16 при сжатии до 3 бит.\n",
                "\n",
                "### 1.3. SnapKV / Context Pruning (2026)\n",
                "Для моделей с контекстом 1M+ токенов даже 2-битный KV-кэш становится слишком тяжелым. \n",
                "**SnapKV** анализирует паттерны внимания и обнаруживает, что для генерации следующего токена реально нужны менее 5% предыдущих ключей. Остальные 95% кэша динамически «выбрасываются» (evicted) без потери смысла.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bitnet_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "def bitnet_quantize_raw(w):\n",
                "    \"\"\" Реализация логики BitNet b1.58 ({-1, 0, 1}) \"\"\"\n",
                "    scale = w.abs().mean()\n",
                "    # Квантуем в {-1, 0, 1}\n",
                "    w_q = torch.round(w / (scale + 1e-6)).clamp(-1, 1)\n",
                "    return w_q, scale\n",
                "\n",
                "class BitLinear(nn.Linear):\n",
                "    def forward(self, x):\n",
                "        # В BitNet активации квантуются в INT8, а веса в 1.58 бит\n",
                "        w_q, scale = bitnet_quantize_raw(self.weight)\n",
                "        # Вместо умножения здесь могла бы быть оптимизированная сумма\n",
                "        return torch.nn.functional.linear(x, w_q * scale, self.bias)\n",
                "\n",
                "print(\"nanoGPT Track: Реализован прототип BitLinear (1.58-bit).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "snapkv_impl",
            "metadata": {},
            "outputs": [],
            "source": [
                "def snapkv_prune_cache_raw(key_cache, attn_scores, keep_ratio=0.1):\n",
                "    \"\"\" Симуляция SnapKV: оставляем только самые важные ключи в кеше \"\"\"\n",
                "    # attn_scores: (B, H, T, T) - усредненные веса внимания\n",
                "    importance = attn_scores.mean(dim=(0, 1, 2)) # Важность каждого токена\n",
                "    k = int(len(importance) * keep_ratio)\n",
                "    \n",
                "    top_indices = torch.topk(importance, k).indices\n",
                "    pruned_cache = key_cache[:, :, top_indices, :]\n",
                "    return pruned_cache\n",
                "\n",
                "print(\"nanoGPT Track: Реализована логика Context Pruning (SnapKV).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "sota_industrial_desc",
            "metadata": {},
            "source": [
                "## 2. Промышленная реализация: BitNet-a & Speculative Decoding\n",
                "В 2026 году Microsoft и Hugging Face выпустили библиотеки для работы с нативными 1-битными весами. \n",
                "Одновременно с этим, **Speculative Decoding** (через библиотеки `vLLM` или `TensorRT-LLM`) стал обязательным: сжатая модель (Draft) предсказывает 5-10 токенов вперед, а тяжелая модель (Target) проверяет их за один проход."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sota_industrial_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"\n",
                "# Пример Speculative Decoding в vLLM [2026]\n",
                "from vllm import LLM, SamplingParams\n",
                "\n",
                "llm = LLM(\n",
                "    model=\"deepseek-67b\",\n",
                "    speculative_model=\"deepseek-7b-quantized\", # Наша сжатая модель\n",
                "    num_speculative_tokens=5\n",
                ")\n",
                "\"\"\"\n",
                "print(\"Llama Track: Speculative Decoding ускоряет инференс в 3 раза без потери точности.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}