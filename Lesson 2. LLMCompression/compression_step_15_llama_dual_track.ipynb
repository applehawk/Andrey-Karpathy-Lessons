{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "dual1",
            "metadata": {},
            "source": [
                "# Этап 15: Dual-Track — nanoGPT vs Llama 3.2\n",
                "\n",
                "Теперь мы переходим на «две колеи». Мы будем применять одни и те же математические трюки к нашей учебной модели nanoGPT и к реальной промышленной модели Llama 3.2 (версия 1B).\n",
                "\n",
                "### Зачем это нужно?\n",
                "1. **Архитектурные различия:** Увидим, как RoPE (Rotary Embeddings) и GQA (Grouped Query Attention) влияют на чувствительность к сжатию.\n",
                "2. **Проверка теории:** Работает ли Wanda одинаково хорошо на модели с 10M и 1B параметров?\n",
                "3. **Практика:** Поймем, как искать нужные слои в сложных иерархиях Hugging Face."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from src.model import GPTLanguageModel, device\n",
                "\n",
                "# 1. Загружаем nanoGPT (Учебная модель)\n",
                "nanogpt = GPTLanguageModel().to(device)\n",
                "nanogpt.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "print(f\"nanoGPT загружена. Параметров: {sum(p.numel() for p in nanogpt.parameters())/1e6:.1f}M\")\n",
                "\n",
                "# 2. Загружаем Llama 3.2 1B (Промышленная модель)\n",
                "# Примечание: Мы используем модель 1B, чтобы она влезла в память ноутбука\n",
                "model_id = \"meta-llama/Llama-3.2-1B\"\n",
                "try:\n",
                "    # Вы можете использовать любую небольшую модель, если нет доступа к Llama\n",
                "    # Например: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "    llama = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
                "    print(f\"Llama 3.2 загружена. Параметров: {sum(p.numel() for p in llama.parameters())/1e9:.1f}B\")\n",
                "except Exception as e:\n",
                "    print(f\"Ошибка загрузки Llama (возможно, нужен логин в HF): {e}\")\n",
                "    print(\"Используем TinyLlama как замену...\")\n",
                "    llama = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16).to(device)\n",
                "    print(\"TinyLlama загружена.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dual3",
            "metadata": {},
            "source": [
                "### Сравнение структуры слоев\n",
                "\n",
                "Давайте найдем аналогичные слои в обеих моделях. Нам нужно понять, куда «бить» нашими методами сжатия."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def print_layer_structure(name, model):\n",
                "    print(f\"\\n--- Структура {name} ---\")\n",
                "    # Выведем первые несколько слоев для примера\n",
                "    for i, (n, p) in enumerate(model.named_modules()):\n",
                "        if i > 15: break # Ограничим вывод\n",
                "        print(f\"{n:<40} | {type(p).__name__}\")\n",
                "\n",
                "print_layer_structure(\"nanoGPT\", nanogpt)\n",
                "print_layer_structure(\"Llama\", llama)\n",
                "\n",
                "print(\"\\nЗаметили разницу? \\nIn nanoGPT: blocks.0.attn.c_attn\")\n",
                "print(\"In Llama: model.layers.0.self_attn.q_proj (отдельные матрицы для Q, K, V!)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dual5",
            "metadata": {},
            "source": [
                "### Задача на этот этап:\n",
                "Мы реализуем **Pruning (зануление весов)** параллельно для обеих моделей вручную, без использования `peft`.\n",
                "\n",
                "1. В nanoGPT мы занулим 50% весов в `c_attn`.\n",
                "2. В Llama мы занулим 50% весов в `q_proj`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dual6",
            "metadata": {},
            "outputs": [],
            "source": [
                "def manual_prune(layer, amount=0.5):\n",
                "    with torch.no_grad():\n",
                "        w = layer.weight.data\n",
                "        # Простое магнитудное зануление\n",
                "        threshold = torch.quantile(w.abs(), amount)\n",
                "        mask = w.abs() > threshold\n",
                "        layer.weight.data *= mask\n",
                "        return mask\n",
                "\n",
                "# 1. Прунинг nanoGPT\n",
                "nano_layer = nanogpt.blocks[0].attn.c_attn\n",
                "manual_prune(nano_layer)\n",
                "print(\"nanoGPT: Слой attn.c_attn обрезан на 50%\")\n",
                "\n",
                "# 2. Прунинг Llama\n",
                "llama_layer = llama.model.layers[0].self_attn.q_proj\n",
                "manual_prune(llama_layer)\n",
                "print(\"Llama: Слой layers.0.self_attn.q_proj обрезан на 50%\")\n",
                "\n",
                "print(\"\\nМатематика одна — масштаб разный. Теперь вы готовы применять SOTA-методы к любой модели!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}