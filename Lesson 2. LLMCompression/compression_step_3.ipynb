{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "q1",
            "metadata": {},
            "source": [
                "# –≠—Ç–∞–ø 3: INT8 Quantization ‚Äî –ü–æ–ª–Ω—ã–π —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
                "\n",
                "–ú—ã —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–≤–µ –º–æ–¥–µ–ª–∏ –ø–æ —Ç—Ä–µ–º –∫–ª—é—á–µ–≤—ã–º –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∞–º:\n",
                "1. **Memory (–ü–∞–º—è—Ç—å)**: –§–∏–∑–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–º –∑–∞–Ω–∏–º–∞–µ–º–æ–≥–æ –º–µ—Å—Ç–∞.\n",
                "2. **Throughput (–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å)**: –°–∫–æ–ª—å–∫–æ —Å–∏–º–≤–æ–ª–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è –≤ —Å–µ–∫—É–Ω–¥—É.\n",
                "3. **Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)**: –í—Ä–µ–º—è –¥–æ –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–∞–º–æ–≥–æ –ø–µ—Ä–≤–æ–≥–æ —Å–∏–º–≤–æ–ª–∞ (Time to First Token)."
            ]
        },
        {
            "cell_type": "code",
            "id": "q_utils",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import copy\n",
                "import time\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from src.model import GPTLanguageModel, device, get_batch, estimate_loss, decode, encode\n",
                "\n",
                "def get_model_size_mb(mdl, real_int8=False):\n",
                "    if not real_int8:\n",
                "        param_size = sum(p.nelement() * p.element_size() for p in mdl.parameters())\n",
                "        buffer_size = sum(b.nelement() * b.element_size() for b in mdl.buffers())\n",
                "        return (param_size + buffer_size) / 1024**2\n",
                "    else:\n",
                "        total_bits = 0\n",
                "        for name, param in mdl.named_parameters():\n",
                "            bits = 8 if ('weight' in name and param.dim() > 1) else 32\n",
                "            total_bits += param.nelement() * bits\n",
                "        for b in mdl.buffers():\n",
                "            total_bits += b.nelement() * 32\n",
                "        return total_bits / (8 * 1024**2)\n",
                "\n",
                "@torch.no_grad()\n",
                "def measure_performance(mdl, num_tokens=50):\n",
                "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
                "    \n",
                "    # Warmup\n",
                "    _ = mdl.generate(context, max_new_tokens=5)\n",
                "    \n",
                "    # 1. Latency (TTFT)\n",
                "    start_latency = time.time()\n",
                "    _ = mdl.generate(context, max_new_tokens=1)\n",
                "    latency = (time.time() - start_latency) * 1000\n",
                "    \n",
                "    # 2. Throughput\n",
                "    start_throughput = time.time()\n",
                "    _ = mdl.generate(context, max_new_tokens=num_tokens)\n",
                "    duration = time.time() - start_throughput\n",
                "    throughput = num_tokens / duration\n",
                "    \n",
                "    return latency, throughput\n",
                "\n",
                "def quantize_tensor_int8(x):\n",
                "    x_max = x.abs().max().item()\n",
                "    if x_max == 0: return x\n",
                "    scale = x_max / 127.0\n",
                "    return torch.round(x / scale).clamp(-128, 127) * scale"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3",
            "metadata": {},
            "source": [
                "### 1. –ó–∞–º–µ—Ä –∏—Å—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ (Baseline FP32)\n",
                "–ó–¥–µ—Å—å –º—ã —Ñ–∏–∫—Å–∏—Ä—É–µ–º ¬´–∑–æ–ª–æ—Ç–æ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç¬ª –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏."
            ]
        },
        {
            "cell_type": "code",
            "id": "q4",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_fp32 = GPTLanguageModel().to(device)\n",
                "try:\n",
                "    model_fp32.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "    print(\"‚úÖ –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏.\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∑–∞–º–µ—Ä—ã –±—É–¥—É—Ç –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ—Å–∞—Ö.\")\n",
                "model_fp32.eval()\n",
                "\n",
                "print(\"\\n--- [STEP 1] Baseline FP32 Performance ---\")\n",
                "loss_fp32 = estimate_loss(model_fp32)['val'].item()\n",
                "size_fp32 = get_model_size_mb(model_fp32)\n",
                "lat_fp32, thr_fp32 = measure_performance(model_fp32)\n",
                "\n",
                "print(f\"üîπ Memory Usage:     {size_fp32:.2f} MB\")\n",
                "print(f\"üîπ Inference Latency: {lat_fp32:.2f} ms (Time to First Token)\")\n",
                "print(f\"üîπ Throughput Rate:   {thr_fp32:.2f} tokens/sec\")\n",
                "print(f\"üîπ Model Quality:     {loss_fp32:.4f} (Val Loss)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5",
            "metadata": {},
            "source": [
                "### 2. –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–º–µ—Ä INT8\n",
                "–¢–µ–ø–µ—Ä—å –º—ã ¬´–ø–æ—Ä—Ç–∏–º¬ª –≤–µ—Å–∞ –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º –¥–æ 8 –±–∏—Ç –∏ —Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –∏–∑–º–µ–Ω—è—Ç—Å—è —Ç–µ –∂–µ –º–µ—Ç—Ä–∏–∫–∏."
            ]
        },
        {
            "cell_type": "code",
            "id": "q6",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_int8 = copy.deepcopy(model_fp32)\n",
                "with torch.no_grad():\n",
                "    for name, param in model_int8.named_parameters():\n",
                "        if 'weight' in name and param.dim() > 1:\n",
                "            param.copy_(quantize_tensor_int8(param.data))\n",
                "\n",
                "print(\"--- [STEP 2] Quantized INT8 Performance ---\")\n",
                "loss_int8 = estimate_loss(model_int8)['val'].item()\n",
                "size_int8 = get_model_size_mb(model_int8, real_int8=True)\n",
                "lat_int8, thr_int8 = measure_performance(model_int8)\n",
                "\n",
                "print(f\"üî∏ Memory Usage:     {size_int8:.2f} MB (Estimated storage size)\")\n",
                "print(f\"üî∏ Inference Latency: {lat_int8:.2f} ms\")\n",
                "print(f\"üî∏ Throughput Rate:   {thr_int8:.2f} tokens/sec\")\n",
                "print(f\"üî∏ Model Quality:     {loss_int8:.4f} (Val Loss)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q7",
            "metadata": {},
            "source": [
                "### 3. –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
                "–°—Ä–∞–≤–Ω–∏–º –≤—ã–∏–≥—Ä—ã—à –≤ —Ä–µ—Å—É—Ä—Å–∞—Ö –ø—Ä–æ—Ç–∏–≤ –ø–æ—Ç–µ—Ä–∏ –∫–∞—á–µ—Å—Ç–≤–∞."
            ]
        },
        {
            "cell_type": "code",
            "id": "q8",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = {\n",
                "    \"Metric\": [\"Memory (MB)\", \"Throughput (tokens/s)\", \"Latency (ms)\", \"Validation Loss\"],\n",
                "    \"FP32\": [size_fp32, thr_fp32, lat_fp32, loss_fp32],\n",
                "    \"INT8\": [size_int8, thr_int8, lat_int8, loss_int8],\n",
                "    \"Delta\": [\n",
                "        f\"{size_fp32/size_int8:.1f}x smaller\", \n",
                "        f\"{(thr_int8/thr_fp32 - 1)*100:+.1f}% check\", \n",
                "        f\"{lat_int8 - lat_fp32:+.2f} ms\", \n",
                "        f\"{(loss_int8/loss_fp32 - 1)*100:+.2f}% quality loss\"\n",
                "    ]\n",
                "}\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "display(df)\n",
                "\n",
                "print(f\"\\nüöÄ –ò–¢–û–ì: –í—ã –æ—Å–≤–æ–±–æ–¥–∏–ª–∏ {(size_fp32 - size_int8):.2f} MB –ø–∞–º—è—Ç–∏!\")\n",
                "print(f\"üìâ –ü–æ—Ç–µ—Ä—è –∫–∞—á–µ—Å—Ç–≤–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ –≤—Å–µ–≥–æ {((loss_int8/loss_fp32 - 1)*100):.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q10_md",
            "metadata": {},
            "source": [
                "### 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (Blind Test)\n",
                "–ù–∞–ø–∏—à–µ–º –ø—Ä–æ–º–ø—Ç –∏ –ø–æ—Å–º–æ—Ç—Ä–∏–º —Ä–∞–∑–Ω–∏—Ü—É –≤ —Å—Ç–∏–ª–µ. \n",
                "*–ï—Å–ª–∏ –≤—ã –≤–∏–¥–∏—Ç–µ NameError, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—ã–ø–æ–ª–Ω–∏–ª–∏ —Å–∞–º—É—é –ø–µ—Ä–≤—É—é —è—á–µ–π–∫—É –∫–æ–¥–∞.*"
            ]
        },
        {
            "cell_type": "code",
            "id": "q10",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.model import decode, encode\n",
                "\n",
                "prompt = \"ROMEO: \"\n",
                "context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
                "\n",
                "print(\"--- FP32 OUTPUT ---\")\n",
                "print(decode(model_fp32.generate(context, max_new_tokens=100)[0].tolist()))\n",
                "\n",
                "print(\"\\n--- INT8 OUTPUT ---\")\n",
                "print(decode(model_int8.generate(context, max_new_tokens=100)[0].tolist()))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.13 (nanoGPT)",
            "language": "python",
            "name": "nanogpt"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}