{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "l1",
            "metadata": {},
            "source": [
                "# Этап 5: LoRA (Low-Rank Adaptation) — Хирургическое дообучение\n",
                "\n",
                "LoRA позволяет адаптировать модель под новую задачу, обучая менее 1% её параметров. \n",
                "\n",
                "### Математическая идея:\n",
                "Мы представляем изменение весов $\\Delta W$ как произведение двух матриц низкого ранга:\n",
                "$$\\Delta W = B \\cdot A$$\n",
                "Где $A \\in \\mathbb{R}^{r \\times k}$ и $B \\in \\mathbb{R}^{d \\times r}$, а ранг $r$ очень мал (например, 2, 4 или 8).\n",
                "\n",
                "**Итоговый выход слоя:**\n",
                "$$h = W x + \\frac{\\alpha}{r} (B A) x$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "l2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: mps\n",
                        "✅ LoRA-слой готов к использованию!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from src.model import GPTLanguageModel, device, decode, encode\n",
                "import copy\n",
                "\n",
                "# 1. Реализация LoRA-слоя для линейной проекции\n",
                "class LoRALinear(nn.Module):\n",
                "    def __init__(self, linear_layer, rank=4, alpha=8):\n",
                "        super().__init__()\n",
                "        self.linear = linear_layer\n",
                "        self.rank = rank\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # Замораживаем веса основного слоя\n",
                "        self.linear.weight.requires_grad = False\n",
                "        if self.linear.bias is not None:\n",
                "            self.linear.bias.requires_grad = False\n",
                "            \n",
                "        # Инициализируем A и B матрицы\n",
                "        in_features = self.linear.in_features\n",
                "        out_features = self.linear.out_features\n",
                "        \n",
                "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
                "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
                "        self.scaling = alpha / rank\n",
                "\n",
                "    def forward(self, x):\n",
                "        # Основной путь (замороженный)\n",
                "        base_out = self.linear(x)\n",
                "        \n",
                "        # Путь LoRA (обучаемый)\n",
                "        # (B, T, in) @ (in, rank) @ (rank, out)\n",
                "        lora_out = (x @ self.lora_A) @ self.lora_B\n",
                "        \n",
                "        return base_out + lora_out * self.scaling\n",
                "\n",
                "print(\"✅ LoRA-слой готов к использованию!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l3",
            "metadata": {},
            "source": [
                "### 2. Внедрение LoRA в вашу модель\n",
                "Мы заменим все слои `query` и `value` в блоках внимания на их LoRA-версии."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "l4",
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_lora(model, rank=4):\n",
                "    # Итерируемся по компонентам модели\n",
                "    for block in model.blocks:\n",
                "        for head in block.sa.heads:\n",
                "            # Заменяем query и value\n",
                "            head.query = LoRALinear(head.query, rank=rank)\n",
                "            head.value = LoRALinear(head.value, rank=rank)\n",
                "    return model\n",
                "\n",
                "base_model = GPTLanguageModel().to(device)\n",
                "base_model.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "\n",
                "lora_model = apply_lora(base_model, rank=4)\n",
                "\n",
                "# Считаем параметры\n",
                "total_params = sum(p.numel() for p in lora_model.parameters())\n",
                "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
                "\n",
                "print(f\"Всего параметров: {total_params:,}\")\n",
                "print(f\"Обучаемых параметров (LoRA): {trainable_params:,}\")\n",
                "print(f\"Доля обучаемых параметров: {100 * trainable_params / total_params:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l5",
            "metadata": {},
            "source": [
                "### 3. Блиц-дообучение на новой задаче\n",
                "Давайте заставим Шекспира выучить фразу про роботов и искусственный интеллект."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "l6",
            "metadata": {},
            "outputs": [],
            "source": [
                "fine_tune_text = \"AI ROBOT: To be or not to be a machine, that is the computation! Machines will rule the code.\\n\" * 50\n",
                "data_ft = torch.tensor(encode(fine_tune_text), dtype=torch.long, device=device)\n",
                "\n",
                "# Простой цикл дообучения\n",
                "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=1e-3)\n",
                "lora_model.train()\n",
                "\n",
                "print(\"Начинаем LoRA-финтюнинг...\")\n",
                "for i in range(200):\n",
                "    # Берем случайный кусок из текста про роботов\n",
                "    ix = torch.randint(len(data_ft) - 256, (16,))\n",
                "    x = torch.stack([data_ft[j:j+256] for j in ix])\n",
                "    y = torch.stack([data_ft[j+1:j+256+1] for j in ix])\n",
                "    \n",
                "    logits, loss = lora_model(x, y)\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if i % 40 == 0:\n",
                "        print(f\"Шаг {i}, Loss: {loss.item():.4f}\")\n",
                "\n",
                "print(\"Дообучение завершено!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l7",
            "metadata": {},
            "source": [
                "### 4. Проверка результата\n",
                "Теперь проверим, как наш Шекспир объединяет свои старые знания с новыми LoRA-адаптерами."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "l8",
            "metadata": {},
            "outputs": [],
            "source": [
                "lora_model.eval()\n",
                "context = torch.tensor(encode(\"KING: \"), dtype=torch.long, device=device).unsqueeze(0)\n",
                "print(\"--- Output AFTER LoRA Fine-tuning ---\")\n",
                "print(decode(lora_model.generate(context, max_new_tokens=150)[0].tolist()))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
