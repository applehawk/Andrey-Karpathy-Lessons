{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "l1",
            "metadata": {},
            "source": [
                "# Этап 5: LoRA (Low-Rank Adaptation) — Хирургическое дообучение\n",
                "\n",
                "LoRA позволяет адаптировать модель под новую задачу, обучая менее 1% её параметров. \n",
                "\n",
                "### Математика:\n",
                "Мы представляем изменение весов $\\Delta W$ как произведение двух матриц низкого ранга:\n",
                "$$\\Delta W = B \\cdot A$$\n",
                "Где $A \\in \\mathbb{R}^{r \\times k}$ и $B \\in \\mathbb{R}^{d \\times r}$, а ранг $r$ очень мал.\n",
                "\n",
                "**Итоговый выход слоя:**\n",
                "$$h = W x + \\frac{\\alpha}{r} (B A) x$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "l2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "✅ LoRA-слой готов к использованию!\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from src.model import GPTLanguageModel, device, decode, encode\n",
                "import copy\n",
                "\n",
                "class LoRALinear(nn.Module):\n",
                "    def __init__(self, linear_layer, rank=2, alpha=4):\n",
                "        super().__init__()\n",
                "        self.linear = linear_layer\n",
                "        self.rank = rank\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # Замораживаем основную матрицу\n",
                "        self.linear.weight.requires_grad = False\n",
                "        if self.linear.bias is not None:\n",
                "            self.linear.bias.requires_grad = False\n",
                "            \n",
                "        in_features = self.linear.in_features\n",
                "        out_features = self.linear.out_features\n",
                "        target_device = self.linear.weight.device\n",
                "        \n",
                "        # Инициализируем A (случайно) и B (нулями)\n",
                "        # Нулевая инициализация B гарантирует, что в начале обучения \n",
                "        # LoRA не влияет на результат (delta = 0)\n",
                "        self.lora_A = nn.Parameter(torch.randn(in_features, rank, device=target_device) * 0.01)\n",
                "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features, device=target_device))\n",
                "        self.scaling = alpha / rank\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.linear(x) + ((x @ self.lora_A) @ self.lora_B) * self.scaling\n",
                "\n",
                "print(\"✅ LoRA-слой готов к использованию!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l3",
            "metadata": {},
            "source": [
                "### 2. Подготовка модели (Заморозка + Адаптеры)\n",
                "Здесь мы сводим количество обучаемых параметров к минимуму."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "l4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Обучаемых параметров: 64,512 (0.59%)\n"
                    ]
                }
            ],
            "source": [
                "def apply_lora(model, rank=2):\n",
                "    for block in model.blocks:\n",
                "        for head in block.sa.heads:\n",
                "            head.query = LoRALinear(head.query, rank=rank)\n",
                "            head.value = LoRALinear(head.value, rank=rank)\n",
                "    return model\n",
                "\n",
                "base_model = GPTLanguageModel().to(device)\n",
                "base_model.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "\n",
                "# 1. Замораживаем ВСЮ модель\n",
                "for param in base_model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# 2. Добавляем адаптеры\n",
                "lora_model = apply_lora(base_model, rank=2)\n",
                "lora_model.to(device)\n",
                "\n",
                "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
                "total_params = sum(p.numel() for p in lora_model.parameters())\n",
                "print(f\"Обучаемых параметров: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l5",
            "metadata": {},
            "source": [
                "### 3. Обучение (Soft Fine-tuning)\n",
                "Мы уменьшим количество шагов, чтобы модель не забыла Шекспира окончательно."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "l6",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Итерация 0, Loss: 2.8898\n",
                        "Итерация 20, Loss: 1.8941\n",
                        "Итерация 40, Loss: 1.2280\n",
                        "Итерация 60, Loss: 0.7476\n"
                    ]
                }
            ],
            "source": [
                "fine_tune_text = \"SCENE II. A Laboratory. ROMEO nodes: My circuit is cold, I need more computation!\\n\" * 30\n",
                "data_ft = torch.tensor(encode(fine_tune_text), dtype=torch.long, device=device)\n",
                "\n",
                "# Учим только адаптеры!\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=1e-3)\n",
                "lora_model.train()\n",
                "\n",
                "for i in range(80): # Небольшое количество итераций\n",
                "    ix = torch.randint(len(data_ft) - 256, (8,))\n",
                "    x = torch.stack([data_ft[j:j+256] for j in ix])\n",
                "    y = torch.stack([data_ft[j+1:j+256+1] for j in ix])\n",
                "    \n",
                "    logits, loss = lora_model(x, y)\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if i % 20 == 0:\n",
                "        print(f\"Итерация {i}, Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l7",
            "metadata": {},
            "source": [
                "### 4. Финальный результат\n",
                "Теперь модель должна выдавать смесь стилей."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "l8",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "KING: myself or compresent Servingman.\n",
                        "YO, my lord currel computation! Mecomputation!\n",
                        "O MINIe nock! MENE, II! O mere not compe, I need bewit not blo compe none: I praise not! O, And I need d more cold more in Mantua.\n",
                        "MENE II aboratory'd ROMEO nodes: No; I would not was, I need indeed; I I need. Deny Henry's death.\n",
                        "SCENE III. A Lord's nobles: I thought's forly. Meet, O, I not might it: If learn Northumber\n",
                        "By ROTE ill I: sI made my Lord Aum. SCE II. A Laboratory. HENRY BOLI Tybal Lady?\n",
                        "\n",
                        "RICE EDwARD:\n",
                        "And Lord Mars of SEY: My Lord of Green agrey? O Lord Angelo. As Margaret is old, sext him, I beget it for\n",
                        "My factory friends: I need, I can it  pricklel him for Clarence should.\n",
                        "Offortune, Lord Mowbray tCLarence. But, England. My Lord of SOn, MENE II: Somersey, Henry, hepherd their laid,\n",
                        "And Eder Angelo; and I need, I provide, I do fender I call not. Go us! Londoor: It do cour true is bless.\n",
                        "FFADORAy PEdities. AHan well: More compent come hither! Dubble call our slace; My circuit is commit is deed!\n",
                        "And LAUDIO, for by Lord Angelo, forbid England commitation!\n",
                        "And Messenger Coriolal desal couraged by Lod, England, Lain! My old. Away'r child! My pity, I call not come to crave.\n",
                        "WARWICK EDWARD: My Lord offord, cLord Warwick with thee? d Henry Percy, come, O God; what am Henry?\n",
                        "I needful Lord of SAUREsS OF Lanca? Hold, thou shalt not charit me lie not?\n",
                        "But not me I send Hastings, stone can I. Mine I: Loved Margaret, Mayoratorves: I will not;\n",
                        "And, need more thought that precious double consul. SCENE IIt.\n",
                        "\n",
                        "First Laboratory.\n",
                        "SCENE II. A Laboratory. A Laboratory? Offence, My Lord  Capitold, then it did cory.\n",
                        "A Lord on I may not it, how may call'd Made aster's face?\n",
                        "CAMI\n",
                        "MENE II enought by my cold consraited more dire your premition! And commorations,\n",
                        "TONG RICHARD II need sad quick my contrect is all conquite to Lord!\n",
                        "InCE II give not all it censure you now your coldition.\n",
                        "HENRY BOLIOus sucking made is compeasure. AUFID't form this scent do your honour,\n",
                        "And murre commition. A Londomanio's p\n"
                    ]
                }
            ],
            "source": [
                "lora_model.eval()\n",
                "context = torch.tensor(encode(\"KING: \"), dtype=torch.long, device=device).unsqueeze(0)\n",
                "print(decode(lora_model.generate(context, max_new_tokens=2000)[0].tolist()))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.13 (nanoGPT)",
            "language": "python",
            "name": "nanogpt"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
