{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "l1",
            "metadata": {},
            "source": [
                "# Этап 5: LoRA (Low-Rank Adaptation) — Хирургическое дообучение\n",
                "\n",
                "LoRA позволяет адаптировать модель под новую задачу, обучая менее 1% её параметров. \n",
                "\n",
                "### Математика:\n",
                "Мы представляем изменение весов $\\Delta W$ как произведение двух матриц низкого ранга:\n",
                "$$\\Delta W = B \\cdot A$$\n",
                "Где $A \\in \\mathbb{R}^{r \\times k}$ и $B \\in \\mathbb{R}^{d \\times r}$, а ранг $r$ очень мал.\n",
                "\n",
                "**Итоговый выход слоя:**\n",
                "$$h = W x + \\frac{\\alpha}{r} (B A) x$$"
            ]
        },
        {
            "cell_type": "code",
            "id": "l2",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from src.model import GPTLanguageModel, device, decode, encode\n",
                "import copy\n",
                "\n",
                "class LoRALinear(nn.Module):\n",
                "    def __init__(self, linear_layer, rank=2, alpha=4):\n",
                "        super().__init__()\n",
                "        self.linear = linear_layer\n",
                "        self.rank = rank\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # Замораживаем основную матрицу\n",
                "        self.linear.weight.requires_grad = False\n",
                "        if self.linear.bias is not None:\n",
                "            self.linear.bias.requires_grad = False\n",
                "            \n",
                "        in_features = self.linear.in_features\n",
                "        out_features = self.linear.out_features\n",
                "        target_device = self.linear.weight.device\n",
                "        \n",
                "        # Инициализируем A (случайно) и B (нулями)\n",
                "        # Нулевая инициализация B гарантирует, что в начале обучения \n",
                "        # LoRA не влияет на результат (delta = 0)\n",
                "        self.lora_A = nn.Parameter(torch.randn(in_features, rank, device=target_device) * 0.01)\n",
                "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features, device=target_device))\n",
                "        self.scaling = alpha / rank\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.linear(x) + ((x @ self.lora_A) @ self.lora_B) * self.scaling\n",
                "\n",
                "print(\"✅ LoRA-слой готов к использованию!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l3",
            "metadata": {},
            "source": [
                "### 2. Подготовка модели (Заморозка + Адаптеры)\n",
                "Здесь мы сводим количество обучаемых параметров к минимуму."
            ]
        },
        {
            "cell_type": "code",
            "id": "l4",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_lora(model, rank=2):\n",
                "    for block in model.blocks:\n",
                "        for head in block.sa.heads:\n",
                "            head.query = LoRALinear(head.query, rank=rank)\n",
                "            head.value = LoRALinear(head.value, rank=rank)\n",
                "    return model\n",
                "\n",
                "base_model = GPTLanguageModel().to(device)\n",
                "base_model.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "\n",
                "# 1. Замораживаем ВСЮ модель\n",
                "for param in base_model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# 2. Добавляем адаптеры\n",
                "lora_model = apply_lora(base_model, rank=2)\n",
                "lora_model.to(device)\n",
                "\n",
                "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
                "total_params = sum(p.numel() for p in lora_model.parameters())\n",
                "print(f\"Обучаемых параметров: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l5",
            "metadata": {},
            "source": [
                "### 3. Обучение (Soft Fine-tuning)\n",
                "Мы уменьшим количество шагов, чтобы модель не забыла Шекспира окончательно."
            ]
        },
        {
            "cell_type": "code",
            "id": "l6",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fine_tune_text = \"SCENE II. A Laboratory. ROMEO nodes: My circuit is cold, I need more computation!\\n\" * 30\n",
                "data_ft = torch.tensor(encode(fine_tune_text), dtype=torch.long, device=device)\n",
                "\n",
                "# Учим только адаптеры!\n",
                "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, lora_model.parameters()), lr=1e-3)\n",
                "lora_model.train()\n",
                "\n",
                "for i in range(80): # Небольшое количество итераций\n",
                "    ix = torch.randint(len(data_ft) - 256, (8,))\n",
                "    x = torch.stack([data_ft[j:j+256] for j in ix])\n",
                "    y = torch.stack([data_ft[j+1:j+256+1] for j in ix])\n",
                "    \n",
                "    logits, loss = lora_model(x, y)\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "    \n",
                "    if i % 20 == 0:\n",
                "        print(f\"Итерация {i}, Loss: {loss.item():.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "l7",
            "metadata": {},
            "source": [
                "### 4. Финальный результат\n",
                "Теперь модель должна выдавать смесь стилей."
            ]
        },
        {
            "cell_type": "code",
            "id": "l8",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "lora_model.eval()\n",
                "context = torch.tensor(encode(\"KING: \"), dtype=torch.long, device=device).unsqueeze(0)\n",
                "print(decode(lora_model.generate(context, max_new_tokens=200)[0].tolist()))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.13 (nanoGPT)",
            "language": "python",
            "name": "nanogpt"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}