{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "q1",
            "metadata": {},
            "source": [
                "# Этап 3: Ручное квантование в INT8 (Naive Quantization)\n",
                "\n",
                "На этом этапе мы реализуем простейшее **линейное квантование**. \n",
                "Идея проста: мы берем диапазон весов слой за слоем и отображаем их из 32-битных чисел (float32) в 8-битные целые числа (int8).\n",
                "\n",
                "### Формула квантования:\n",
                "$$Q(x) = \\text{round}\\left(\\frac{x}{S} + Z\\right)$$\n",
                "где:\n",
                "- $S$ (Scale) — масштаб.\n",
                "- $Z$ (Zero-point) — точка нуля.\n",
                "\n",
                "В нашем примере мы будем использовать **Symmetric Quantization** (для простоты), где $Z=0$."
            ]
        },
        {
            "cell_type": "code",
            "id": "q2",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import copy\n",
                "from src.model import GPTLanguageModel, device, get_batch, estimate_loss\n",
                "\n",
                "# 1. Загружаем оригинал\n",
                "model_fp32 = GPTLanguageModel().to(device)\n",
                "model_fp32.load_state_dict(torch.load('model_ckpt.pt', map_location=device))\n",
                "model_fp32.eval()\n",
                "\n",
                "def get_val_loss(mdl):\n",
                "    with torch.no_grad():\n",
                "        losses = estimate_loss(mdl)\n",
                "    return losses['val'].item()\n",
                "\n",
                "base_loss = get_val_loss(model_fp32)\n",
                "print(f\"Исходный Loss (FP32): {base_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q3",
            "metadata": {},
            "source": [
                "### 2. Реализация линейного квантования\n",
                "Мы напишем функцию, которая имитирует потерю точности при переходе в INT8."
            ]
        },
        {
            "cell_type": "code",
            "id": "q4",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def quantize_tensor_int8(x):\n",
                "    # Вычисляем масштаб (Scale)\n",
                "    # Для 8 бит диапазон [-128, 127]\n",
                "    x_max = x.abs().max().item()\n",
                "    if x_max == 0: return x, 1.0\n",
                "    \n",
                "    scale = x_max / 127.0\n",
                "    \n",
                "    # Квантуем: float -> int8 -> float (dequantize)\n",
                "    # Мы делаем Fake Quantization: оставляем тензор во флоатах, но со значениями, кратными шагу\n",
                "    q_x = torch.round(x / scale).clamp(-128, 127)\n",
                "    dq_x = q_x * scale\n",
                "    \n",
                "    return dq_x\n",
                "\n",
                "# Создаем копию модели для квантования\n",
                "model_int8 = copy.deepcopy(model_fp32)\n",
                "\n",
                "with torch.no_grad():\n",
                "    for name, param in model_int8.named_parameters():\n",
                "        if 'weight' in name and param.dim() > 1:\n",
                "            print(f\"Квантуем слой: {name}\")\n",
                "            param.copy_(quantize_tensor_int8(param.data))\n",
                "\n",
                "new_loss = get_val_loss(model_int8)\n",
                "print(f\"\\nLoss после INT8 квантования: {new_loss:.4f}\")\n",
                "print(f\"Деградация (Delta Loss): {new_loss - base_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q5",
            "metadata": {},
            "source": [
                "### 3. Проверка результата генерацией\n",
                "Посмотрим, не начал ли квантованный Шекспир нести полную чепуху."
            ]
        },
        {
            "cell_type": "code",
            "id": "q6",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.model import decode\n",
                "\n",
                "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
                "print(\"--- Output AFTER INT8 Quantization ---\")\n",
                "print(decode(model_int8.generate(context, max_new_tokens=200)[0].tolist()))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.13 (nanoGPT)",
            "language": "python",
            "name": "nanogpt"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}