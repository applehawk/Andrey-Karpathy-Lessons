Metadata-Version: 2.4
Name: lesson-2-llm-compression
Version: 0.1.0
Summary: Add your description here
Requires-Python: >=3.13
Description-Content-Type: text/markdown
Requires-Dist: ipykernel>=7.1.0
Requires-Dist: matplotlib>=3.10.8
Requires-Dist: notebook>=7.5.3
Requires-Dist: numpy>=2.4.2
Requires-Dist: pandas>=3.0.0
Requires-Dist: ruff>=0.15.0
Requires-Dist: torch>=2.10.0
Requires-Dist: torchaudio>=2.10.0
Requires-Dist: torchvision>=0.25.0
Requires-Dist: transformers
Requires-Dist: accelerate
Requires-Dist: sentencepiece
Requires-Dist: peft
Requires-Dist: bitsandbytes
Requires-Dist: optimum
Requires-Dist: jupyterlab>=4.1.0

# Andrey Karpathy's Neural Networks Lessons

This repository contains my progress and experiments through Andrey Karpathy's deep learning series.

## ðŸ“š Project Structure

- **[Lesson 1: Micrograd](./Lesson%201.%20Micrograd)** - Building a tiny autograd engine from scratch.
- **[Lesson 2: Transformer Decoder Architecture (nanoGPT)](./Lesson%202.%20Transformer%20Decoder%20Architecture%20(nanoGPT))** - Deep dive into GPT architecture, Multi-Head Attention, and LayerNorm stability.
- **[Lesson 3: LLM Compression](./Lesson%203.%20LLM%20Compression)** - Research foundations and large-scale application of compression techniques (Quantization, LoRA, Pruning).

## ðŸ›  Tech Stack

- **Python 3.13**
- **[uv](https://github.com/astral-sh/uv)** - Ultra-fast Python package manager.
- **PyTorch** - For matrix-based operations and LLM experiments.
- **Ruff** - Fast linting and formatting for both Python files and Jupyter Notebooks.
- **direnv** - Automatic virtual environment activation upon entering directory.

## ðŸš€ Getting Started

Each lesson is self-contained with its own dependencies managed via `uv`.

1. **Install uv** (if you haven't):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

2. **Setup a lesson**:
   ```bash
   cd "Lesson 2. nanoGPT"
   make install
   ```

3. **Format code**:
   ```bash
   make format
   ```

## ðŸ“ˆ Current Focus: LLM Compression

I am currently working on Lesson 2, transitioning from basic Transformer implementation to advanced compression techniques:
- [x] Basic GPT Architecture
- [ ] Manual INT8 Quantization
- [ ] LoRA (Low-Rank Adaptation)
- [ ] Pruning & Saliency Analysis
