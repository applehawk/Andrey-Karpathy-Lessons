{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Визуализация взрыва градиентов (Exploding Gradients)\n",
    "\n",
    "В этом эксперименте мы увидим, как неправильная инициализация весов в глубокой сети приводит к тому, что значения активаций и градиентов растут экспоненциально, делая обучение невозможным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simulation_setup",
   "metadata": {},
   "source": [
    "## 1. Создание глубокой линейной сети\n",
    "\n",
    "Мы создадим сеть из 50 последовательных линейных слоев. Мы намеренно инициализируем их веса так, чтобы они были чуть больше единицы ($W \\approx 1.5 \\cdot I$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_def",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 50 # Количество слоев\n",
    "D = 128 # Размерность\n",
    "\n",
    "layers = []\n",
    "for i in range(L):\n",
    "    layer = nn.Linear(D, D, bias=False)\n",
    "    # Намеренная ПЛОХАЯ инициализация: веса чуть больше 1\n",
    "    # Это гарантирует взрыв при умножении матриц друг на друга\n",
    "    with torch.no_grad():\n",
    "        layer.weight.copy_(torch.eye(D) * 1.5 + torch.randn(D, D) * 0.01)\n",
    "    layers.append(layer)\n",
    "\n",
    "model = nn.Sequential(*layers)",
    "\n",
    "# Визуализация структуры параметров\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:40} | Shape: {str(list(param.shape)):20} | Mean: {param.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward_pass",
   "metadata": {},
   "source": [
    "## 2. Forward Pass: Рост активаций\n",
    "\n",
    "Посмотрим, как меняется норма вектора данных по мере прохождения сквозь 50 слоев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward_sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, D)\n",
    "activations_norms = []\n",
    "\n",
    "curr_x = x\n",
    "for layer in model:\n",
    "    curr_x = layer(curr_x)\n",
    "    activations_norms.append(curr_x.norm().item())\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(activations_norms, 'r-o')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Норма активаций (Forward Pass) - Логарифмическая шкала\")\n",
    "plt.xlabel(\"Слой\")\n",
    "plt.ylabel(\"L2 Norm\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Начальная норма: {activations_norms[0]:.2f}\")\n",
    "print(f\"Финальная норма через 50 слоев: {activations_norms[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backward_pass",
   "metadata": {},
   "source": [
    "## 3. Backward Pass: Взрыв градиентов\n",
    "\n",
    "Теперь мы вычислим лосс и сделаем `backward()`. Мы будем записывать норму градиента для каждого слоя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backward_sim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Считаем градиенты\n",
    "loss = curr_x.sum()\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "gradient_norms = []\n",
    "layer_names = []\n",
    "for i, layer in enumerate(model):\n",
    "    norm = layer.weight.grad.norm().item()\n",
    "    gradient_norms.append(norm)\n",
    "    layer_names.append(f'Layer {i}')\n",
    "\n",
    "# 2. Визуализация взрыва\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Левый график: Линейная шкала (тут будет виден сам 'взрыв')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gradient_norms, 'b-')\n",
    "plt.title(\"Взрыв градиентов (Линейная шкала)\")\n",
    "plt.xlabel(\"Номер слоя (0-вход, 50-выход)\")\n",
    "plt.ylabel(\"Frobenius Norm of Gradient\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Правый график: Логарифмическая шкала (тут видна динамика)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gradient_norms, 'g-')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Взрыв градиентов (Log-шкала)\")\n",
    "plt.xlabel(\"Номер слоя\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Градиент последнего слоя (Layer 49): {gradient_norms[-1]:.2f}')\n",
    "print(f'Градиент первого слоя (Layer 0):    {gradient_norms[0]:.2e}')\n",
    "print('\\nОбрати внимание: градиент растет ПРИ ОБРАТНОМ ХОДЕ (от конца к началу),')\n",
    "print('поэтому на Layer 0 он в миллионы раз больше, чем на Layer 49!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fix_header",
   "metadata": {},
   "source": [
    "## 4. Как это исправляет LayerNorm?\n",
    "\n",
    "Если мы добавим LayerNorm между нашими слоями, он будет принудительно 'сжимать' активации обратно к нормальному распределению на каждом шаге."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}